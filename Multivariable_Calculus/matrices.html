<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-06-11 Wed 13:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Matrices</title>
<meta name="author" content="Lin Jiang" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="/style.css" type="text/css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="preamble" class="status">
<div class="breadcrumb"><a href="/">Home</a> <span>â†’</span> <a href="/Multivariable_Calculus/index.html">Multivariable Calculus</a></div>
</div>
<div id="content" class="content">
<h1 class="title">Matrices</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0c91469">1. Matrices</a></li>
<li><a href="#orgba31d1a">2. Special Matrics</a>
<ul>
<li><a href="#org1b038ab">2.1. Identity Matrix</a></li>
<li><a href="#org38826ba">2.2. Orthogonal Matrices</a></li>
<li><a href="#orgb402ee8">2.3. Rotation Matrix</a></li>
<li><a href="#org4d5e810">2.4. Inverse Matrix</a>
<ul>
<li><a href="#org3df1f49">2.4.1. Cofactor Method</a></li>
<li><a href="#org904bc70">2.4.2. 2x2 Matrices</a></li>
<li><a href="#orgbfac3c6">2.4.3. Invertible Matrices</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org0c91469" class="outline-2">
<h2 id="org0c91469"><span class="section-number-2">1.</span> Matrices</h2>
<div class="outline-text-2" id="text-1">
<p>
<b>Matrices</b> are useful because a lot of mathematical concepts can be related by linear systems. For example, consider a change of coordinate systems from \((x_1,x_2,x_3)\) to \((u_1,u_2,u_3)\) according to the following rule:
</p>

\begin{aligned}
u_1 & = 2x_1 + 3x_2 + 3x_3 \\
u_2 &= 2x_1 + 4x_2 + 5x_3 \\
u_3 &= x_1 + x_2 + 2x_3
\end{aligned}

<p>
We can rewrite this using matrices and matrix multiplication:
</p>

<p>
\[\begin{bmatrix} 2 & 3 & 3 \\ 2 & 4 & 5 \\ 1 & 1 & 2 \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix} = \begin{bmatrix}u_1 \\ u_2 \\ u_3 \end{bmatrix}\]
</p>

<p>
Or, conveniently:
</p>

<p>
\[
AX = U
\]
</p>

<p>
This works because matrix multiplication is <i>a dot product between the rows of the first matrix and the columns of the second matrix</i>. This also implies that there are strict limitations on what matrices can be multiplied together. In a matrix product \(AB\), the <i>width of A must equal the height of B</i>. It also follows that <i>matrix multiplication is not commutative</i>.
</p>
</div>
</div>
<div id="outline-container-orgba31d1a" class="outline-2">
<h2 id="orgba31d1a"><span class="section-number-2">2.</span> Special Matrics</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org1b038ab" class="outline-3">
<h3 id="org1b038ab"><span class="section-number-3">2.1.</span> Identity Matrix</h3>
<div class="outline-text-3" id="text-2-1">
<p>
It can be intuitively seen that the n-by-n <b>identity matrix</b> is the matrix of all zeroes except for the diagonal, which is all ones. For example, for a 3 by 3 identity matrix:
</p>

<p>
\[
I_{3\times3} = \begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\] 
</p>
</div>
</div>
<div id="outline-container-org38826ba" class="outline-3">
<h3 id="org38826ba"><span class="section-number-3">2.2.</span> Orthogonal Matrices</h3>
<div class="outline-text-3" id="text-2-2">
<p>
<b>Orthogonal matrices</b> are matrices that satisfy the conditions:
</p>

\begin{aligned}
A^TA &= I \\
AA^T &= I
\end{aligned}

<p>
This means that the dot product of the rows/columns of A with themselves must equal 1, meaning that the lengths of each of the row/column vectors must be 1. Since all the other places in the identity matrix are 0, this means that the dot product of each of the row/column vectors with every other row/column vector equals 0, or they are perpendicular with each other. In other words:
</p>

<ol class="org-ol">
<li>Each row and column are of unit length, and</li>
<li>Each row or column are perpendicular with every other row or column, respectively.</li>
</ol>

<p>
There are two types of orthogonal matrices: <b>rotation matrices</b> and <b>reflection matrices</b>.
</p>
</div>
</div>
<div id="outline-container-orgb402ee8" class="outline-3">
<h3 id="orgb402ee8"><span class="section-number-3">2.3.</span> Rotation Matrix</h3>
<div class="outline-text-3" id="text-2-3">
<p>
A <b>rotation matrix</b> can be used to rotate a matrix in space. For example, we can use the following rotation matrix to rotate vectors by 90-degrees, or map \(\hat{i}\) onto \(\hat{j}\) and vice versa:
</p>

<p>
\[
R = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
\]
</p>

<p>
Notice that if we apply this matrix twice, or \(R^2\), this is the equivalent of rotating by 180 degrees; or, the opposite of the identity matrix. In other words, \(R^2 = -I\).
</p>

<p>
More generally, we can find the rotation matrix \(R\) for rotating any 2D vector \(\begin{bmatrix} a\hat{i} \\ b\hat{j} \end{bmatrix}\) by an angle of \(\theta\). To do this, we shall first consider $\hat{i}$and \(\hat{j}\) separately. We can do this because:
</p>

\begin{aligned}
\hat{i} &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
\hat{j} &= \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\end{aligned}

<p>
Notice that when you multiply \(R\) by each of these unit vectors, only the values in the first column affect \(\hat{i}\), and only the values in the second column affect \(\hat{j}\), with the first row determining the x-component, and the second row determining the y-component.
</p>

<p>
For \(\hat{i}\), using trigonometry to determine the x and y components after rotation:
</p>


<div id="org9b44d98" class="figure">
<p><img src="../static/matrices1.png" alt="matrices1.png" />
</p>
</div>

<p>
Similarly, for \(\hat{j}\):
</p>


<div id="org83db47e" class="figure">
<p><img src="../static/matrices2.png" alt="matrices2.png" />
</p>
</div>

<p>
Therefore, the generalized rotation matrix is:
</p>

\begin{aligned}
R = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}
\end{aligned}
</div>
</div>
<div id="outline-container-org4d5e810" class="outline-3">
<h3 id="org4d5e810"><span class="section-number-3">2.4.</span> Inverse Matrix</h3>
<div class="outline-text-3" id="text-2-4">
<p>
The <b>inverse matrix</b> of \(A\) is defined as a matrix \(M=A^{-1}\) such that:
</p>

\begin{aligned}
AM &= I \\
MA &= I
\end{aligned}

<p>
Realize that for this to work, since \(I\) is a square matrix, for \(A\) to have an inverse it must <i>also be a square matrix</i>. The inverse matrix is useful because given a linear system \(AX=B\), we can solve for the variables \(X\) by \(X=A^{-1}B\):
</p>

\begin{aligned}
AX &= B \\
A^{-1}(AX) &= A^{-1}B \\
X &= A^{-1}B
\end{aligned}
</div>
<div id="outline-container-org3df1f49" class="outline-4">
<h4 id="org3df1f49"><span class="section-number-4">2.4.1.</span> Cofactor Method</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
The <b>cofactor method</b> is an algorithm that we can use to calculate the inverse matrix by hand. It says that:
</p>

<p>
\[
A^{-1} = \frac{1}{\det(A)} \text{ adj}(A)
\]
</p>

<p>
The \(\text{adj}(A)\) is the <b>adjoint matrix</b> of \(A\).
</p>

<p>
Next, we shall illustrate the steps to do so with an example for the inverse matrix of \(A\) such that:
</p>

<p>
\[
A = \begin{bmatrix} 2 & 3 & 3 \\ 2 & 4 & 5 \\ 1 & 1 & 2 \end{bmatrix}
\]
</p>
</div>
<ol class="org-ol">
<li><a id="org460446b"></a>Step 1: Minors<br />
<div class="outline-text-5" id="text-2-4-1-1">
<p>
First, we build a <b>matrix of minors</b>. To do so, we construct a matrix of the same dimensions as \(A\). Each entry in this matrix is equivalent to the <i>determinant formed by deleting the row and column corresponding to that entry</i>, in \(A\). For example, the top left corner of the matrix of minors would be:
</p>

<p>
\[
\left| \begin{matrix} 4 & 5 \\ 1 & 2 \end{matrix} \right|
\]
</p>

<p>
Doing this calculation, we end up with the following matrix of minors:
</p>

<p>
\[\begin{bmatrix} 3 & -1 & -2 \\ 3 & 1 & -1 \\ 3 & 4 & 2 \end{bmatrix}\]
</p>
</div>
</li>
<li><a id="orgf2bc95f"></a>Step 2: Cofactors<br />
<div class="outline-text-5" id="text-2-4-1-2">
<p>
Now, we apply cofactors. We flip the signs in the matrix of minors in the following checkerboard pattern:
</p>

<p>
\[\begin{matrix} + &- & + \\ - & + & - \\ + & - & + \end{matrix}\]
</p>

<p>
The \(+\) indicates we leave the number along, and the \(-\) indicates that we flip the sign. For our example, this leaves us with:
</p>

<p>
\[\begin{bmatrix} 3 & 1 & -2 \\ -3 & 1 & 1 \\ 3 & -4 & 2 \end{bmatrix}\]
</p>
</div>
</li>
<li><a id="org40005a8"></a>Step 3: Transpose<br />
<div class="outline-text-5" id="text-2-4-1-3">
<p>
Next, we transpose the matrix; we switch the rows and columns:
</p>

<p>
\[\begin{bmatrix} 3 & -3 & 3 \\ 1 & 1 & -4 \\ -2 & 1 & 2 \end{bmatrix}\].
</p>

<p>
This is the <b>adjoint matrix</b>.
</p>
</div>
</li>
<li><a id="org81930be"></a>Step 4: Divide by Determinant<br />
<div class="outline-text-5" id="text-2-4-1-4">
<p>
Finally, to get the inverse, we divide by the determinant of \(A\), which is \(3\) in this case. Therefore:
</p>

<p>
\[
A^{-1} = \frac{1}{3} \begin{bmatrix} 3 & -3 & 3 \\ 1 & 1 & -4 \\ -2 & 1 & 2 \end{bmatrix}
\]
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org904bc70" class="outline-4">
<h4 id="org904bc70"><span class="section-number-4">2.4.2.</span> 2x2 Matrices</h4>
<div class="outline-text-4" id="text-2-4-2">
<p>
For 2x2 matrices, it is often simpler to know that the inverse of a matrix is:
</p>

<p>
\[
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}
\]
</p>
</div>
</div>
<div id="outline-container-orgbfac3c6" class="outline-4">
<h4 id="orgbfac3c6"><span class="section-number-4">2.4.3.</span> Invertible Matrices</h4>
<div class="outline-text-4" id="text-2-4-3">
<p>
From our algorithm of cofactors, notice that it doesn't work when the determinant of A in the denominaotr in 0. This means that not all square matrices are invertible, as they must satisfy the condition:
</p>

<p>
\[
A \text{ is invertible } \iff \det(A) \neq 0
\]
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<div class="footer">
               <p>Author: Lin Jiang</p>
             </div>
             <div class="last-modified">
               Last modified: 2025-06-11 13:35
             </div>
</div>
</body>
</html>
